{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Markov Chains and Markov Decision Processes (MDP) \n",
    "\n",
    "This exercise deals with the formal handling of Markov chains and Markov decision processes. \n",
    "\n",
    "## 1) Markov Chain: State Transition\n",
    "The graph shows the working life problem. \n",
    "The nodes show the states.\n",
    "The arrows define the possible transitions to other states and the numbers besides the arrows define the propability of the corresponding transition.\n",
    "If you are for example in the state \"Wake Up\", with 20% probability you go for exercise, with 50% probability you browse social media, with 10% probability you watch a movie, with 10% probabilty you go to work and with 10% probabilty you end up sleeping.\n",
    "\n",
    "Define the state transition probability matrix $\\mathcal{P}_{xx'}$ of the graph shown in the figure below!\n",
    "\n",
    "![](life_problem_graph.png)\n",
    "\n",
    "With $p_k = \\begin{bmatrix}\n",
    "\\text{Pr}_k \\lbrace \\text{Wake Up} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Exercise} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Browse Social Media} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Work} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Watch a Movie}\\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Sleep} \\rbrace \\\\\n",
    "\\end{bmatrix}^\\text{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR ANSWER HERE!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE!! \n",
    "import numpy as np\n",
    "P_xx = np.array([]) # TODO: FILL THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Markov Reward Process: Evaluating States\n",
    "\n",
    "In the following rewards for every state are defined.\n",
    "\n",
    "Given the reward distribution $r_\\mathcal{X}$, calculate the state-values $v_\\mathcal{X}$.  \n",
    "\n",
    "The states are defined by:\n",
    "$\\mathcal{X} = \\left\\lbrace \\begin{matrix}\n",
    "\\text{Wake Up}\\\\\n",
    "\\text{Exercise}\\\\\n",
    "\\text{Browse Social Media}\\\\\n",
    "\\text{Work}\\\\\n",
    "\\text{Watch a Movie}\\\\\n",
    "\\text{Sleep}\\\\\n",
    "\\end{matrix}\n",
    "\\right\\rbrace$\n",
    "\n",
    "The rewards are defined by:\n",
    "$r_\\mathcal{X} = \\begin{bmatrix}\n",
    "+1\\\\\n",
    "+3\\\\\n",
    "-2\\\\\n",
    "+2\\\\\n",
    "+1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The state-value is defined by the state-value Bellman equation: $v_\\mathcal{X} = r_\\mathcal{X} + \\gamma \\mathcal{P}_{xx'} v_\\mathcal{X}$. Assume that $\\gamma = 0.9$ and write a Python program to calculate $v_\\mathcal{X}$. Which state is most promising? Why?\n",
    "\n",
    "Which state is most promising when $\\gamma = 0.1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR ANSWER HERE!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define given parameters\n",
    "gamma_0 = 0.9 # discount factor\n",
    "gamma_1 = 0.1 # discount factor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "P_xx = np.array([]) # TODO: FILL THIS\n",
    "# .....\n",
    "# .....\n",
    "v_X =  np.array([]) # TODO: CALCULATE THIS\n",
    "\n",
    "print(v_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Markov Decision Process: State Transition\n",
    "\n",
    "The graph shows an MDP.\n",
    "The nodes are the states. \n",
    "In every state you can choose between two actions (Relax or Grind). \n",
    "Taken actions impact the state transition probability to the next state.\n",
    "If you, for example, \"Wake Up\" and decide to \"Relax\", there is a 50% chance that you \"Browse Social Media\" and a 50% chance that you \"Watch a Movie\", while if you decide to \"Grind\" there is a 100% chance that you \"Exercise\".\n",
    "\n",
    "Define the Relax state transition probabilitiy $\\mathcal{P}_{xx'}^{u=\\text{Relax}}$ and the Grinding state transition probability $\\mathcal{P}_{xx'}^{u=\\text{Grind}}$ of the graph shown in the figure below.\n",
    "\n",
    "![](action_state_transition_life_problem.png)\n",
    "\n",
    "With $p_k = \\begin{bmatrix}\n",
    "\\text{Pr}_k \\lbrace \\text{Wake Up} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Exercise} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Browse Social Media} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Work} \\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Watch a Movie}\\rbrace \\\\\n",
    "\\text{Pr}_k \\lbrace \\text{Sleep} \\rbrace \\\\\n",
    "\\end{bmatrix}^\\text{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR ANSWER HERE\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{P}_{xx'}^{u=\\text{Relax}}&=\\begin{bmatrix}\n",
    " &  &  &  &    & \\\\\n",
    " &  &  &  &    & \\\\\n",
    " &  &  &  &    & \\\\\n",
    " &  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "\\end{bmatrix}\\\\\n",
    "\\mathcal{P}_{xx'}^{u=\\text{Grind}}&=\\begin{bmatrix}\n",
    "&  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "&  &  &  &  & \\\\ \n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "P_xx_relax = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "P_xx_grind = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Markov Decision Process: Trivial Policy Evaluation\n",
    "\n",
    "The rewards for this problem are defined by:\n",
    "$r_\\mathcal{X} = r_\\mathcal{X}^{u=\\text{Grind}} = r_\\mathcal{X}^{u=\\text{Relax}} = \\begin{bmatrix}\n",
    "-1\\\\\n",
    "1\\\\\n",
    "-1\\\\\n",
    "2\\\\\n",
    "-1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "How can we interprete these rewards?\n",
    "Evaluate both the relax policy and the grind policy using $\\gamma = 0.9$.\n",
    "\n",
    "Bonus question: Can we evaluate the state-value of $\\lbrace x=\\text{Watch a Movie}, u=\\text{Relax}\\rbrace$ for an infinite time horizon without the use of the Bellman equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "P_xx_relax = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "P_xx_grind = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "r_X = np.array([]) # TODO: FILL THIS\n",
    "for P_xx in [P_xx_relax, P_xx_grind]:\n",
    "    # TODO: CALCULATE v_X\n",
    "    print(v_X)\n",
    "    \n",
    "    \n",
    "# Bonus question: Can we evaluate the state-value of {ùë•=Watch a Movie,ùë¢=Relax} for an infinite time horizon without the use of the Bellman equation?\n",
    "# CALCULATE x_4\n",
    "v_4 = None # TODO: CALCULATE v_4\n",
    "print(v_4)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Action-Value Function Evalution\n",
    "\n",
    "Now, the policy is defined by:\n",
    "\\begin{align}\n",
    "\\pi(u_k=\\text{Grind} | x_k)&=\\alpha,\\\\\n",
    "\\pi(u_k=\\text{Relax} | x_k)&=1-\\alpha, \\forall x_k \\in \\mathcal{X}\n",
    "\\end{align}\n",
    "\n",
    "Calculate action-values for the problem as described using the 'fifty-fifty' policy ($\\alpha = 0.5$) according to the Bellman Expectation Equation: $q_\\pi(x_k, u_k) = \\mathcal{R}^u_x + \\gamma \\sum_{x_{k+1} \\in \\mathcal{X}} p^u_{xx'} v_\\pi(x_{k+1})$ $\\forall x_k, u_k \\in \\mathcal{X}, \\mathcal{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5\n",
    "no_states = 6\n",
    "no_actions = 2\n",
    "r_X = np.array([-1, 1, -1, 2, -1, 0]).reshape(-1, 1)\n",
    "q_XU = np.zeros([no_states, no_actions])\n",
    "\n",
    "P_xx_relax = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "P_xx_grind = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "# TODO: YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Markov Decision Problem: Stochastic Policy Evalution\n",
    "\n",
    "Plot the state-value of the states \"Wake Up\" and \"Browse Social Media\" for different $\\alpha$. What do we see? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 6 # dimension of state space\n",
    "no_of_samples = 1000\n",
    "\n",
    "alphas = np.linspace(0, 1, no_of_samples)\n",
    "v_n_alpha = np.zeros([n, no_of_samples])\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5\n",
    "no_states = 6\n",
    "no_actions = 2\n",
    "r_X = np.array([-1, 1, -1, 2, -1, 0]).reshape(-1, 1)\n",
    "q_XU = np.zeros([no_states, no_actions])\n",
    "\n",
    "P_xx_relax = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "P_xx_grind = np.array([]) # TODO: FILL THIS\n",
    "\n",
    "# TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 6])\n",
    "states = [\"Wake Up\", \"Exercise\", \"Browse Social Media\", \"Work\", \"Watch a Movie\", \"Sleep\"]\n",
    "alphas = alphas.flatten()\n",
    "for state, vnalp in zip(states, v_n_alpha):\n",
    "    ls = '--' if state in ['Wake Up', 'Browse Social Media'] else '-'\n",
    "    plt.plot(alphas, vnalp, ls=ls, label=r\"$x=${}\".format(state))\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(r\"$v_\\pi(x)$\")\n",
    "plt.xlim([0, 1])\n",
    "\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
